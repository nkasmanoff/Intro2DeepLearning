{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"This is a challenge, using all the ~knowledge~ I gained learning more about word embeddings and tflearn in \n",
    "the imdb review video, I will attempt this independently on an ign video game review dataset.  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the existing labels is extra credit. The baseline is that you can just convert the labels so that there are only 2 emotions (positive or negative). Ideally you can use an RNN via TFLearn like the one in this example, but I'll accept other types of ML models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences, VocabularyProcessor\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great          4773\n",
      "Good           4741\n",
      "Okay           2945\n",
      "Mediocre       1959\n",
      "Amazing        1804\n",
      "Bad            1269\n",
      "Awful           664\n",
      "Painful         340\n",
      "Unbearable       72\n",
      "Masterpiece      55\n",
      "Disaster          3\n",
      "Name: score_phrase, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select only the two columns we require. Game title and its corresponding emotion\n",
    "dataframe = pd.read_csv('ign.csv')\n",
    "# Fill null values with empty strings\n",
    "dataframe.fillna(value='', inplace=True)\n",
    "\n",
    "print(dataframe.score_phrase.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the required columns for inputs and outputs\n",
    "totalX = dataframe.title\n",
    "totalY = dataframe.score_phrase\n",
    "\n",
    "# Convert the strings in the input into integers corresponding to the dictionary positions\n",
    "# Data is automatically padded so we need to pad_sequences manually\n",
    "vocab_proc = VocabularyProcessor(30)  #I know its about 16 words, but let's expand to 25\n",
    "totalX = np.array(list(vocab_proc.fit_transform(totalX)))\n",
    "\n",
    "# We will have 11 classes in total for prediction, indices from 0 to 10\n",
    "vocab_proc2 = VocabularyProcessor(1)\n",
    "totalY = np.array(list(vocab_proc2.fit_transform(totalY))) - 1\n",
    "# Convert the indices into 11 dimensional vectors\n",
    "totalY = to_categorical(totalY, nb_classes=11)\n",
    "\n",
    "# Split into training and testing data\n",
    "trainX, testX, trainY, testY = train_test_split(totalX, totalY, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the data is in a consistent format with the type used in the lecture. This retains a sequential order, and uses a number to correspond to the vocabulary number. Using this embedding and a RNN, the sequential nature of the text should be taken into acct, along with the word itself, and these vals should be associated with a good or bad video game. Unfortunately title doesn't mean anything, so let's try switching to the description and say that's good or bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the network for classification\n",
    "# Each input has length of 15\n",
    "net = tflearn.input_data([None, 15])\n",
    "# The 15 input word integers are then casted out into 256 dimensions each creating a word embedding.\n",
    "# We assume the dictionary has 10000 words maximum\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=256)\n",
    "# Each input would have a size of 15x256 and each of these 256 sized vectors are fed into the LSTM layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second LSTM layer.\n",
    "net = tflearn.gru(net, 256, dropout=0.9, return_seq=True)\n",
    "# Using the intermediate outputs, we pass them to another LSTM layer and collect the final output only this time\n",
    "net = tflearn.gru(net, 256, dropout=0.9)\n",
    "# The output is then sent to a fully connected layer that would give us our final 11 classes\n",
    "net = tflearn.fully_connected(net, 11, activation='softmax')\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "\t\t\t\t\t\t loss='categorical_crossentropy')\n",
    "\n",
    "# Train the network\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "\n",
    "if load_model == 1:\n",
    "\tmodel.load('gamemodel.tfl')\n",
    "\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True, batch_size=32, n_epoch=20)\n",
    "\n",
    "if save_model == 1:\n",
    "\tmodel.save('gamemodel.tfl')\n",
    "\tprint \"Saved model!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
