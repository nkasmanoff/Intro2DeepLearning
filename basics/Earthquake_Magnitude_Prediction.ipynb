{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to build a neural network to predict the magnitude of an Earthquake given the date, time, Latitude, and Longitude as features. This is the dataset. Optimize at least 1 hyperparameter using Random Search. See this example for more information\n",
    "\n",
    "\n",
    "You can use any library you like, bonus points are given if you do this using only numpy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('database.csv')[['Date','Time','Latitude','Longitude','Magnitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['Magnitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['Latitude','Longitude']].values\n",
    "lons = data['Longitude'].values\n",
    "lats = data['Latitude'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got a weird version\n",
      "got a weird version\n",
      "got a weird version\n"
     ]
    }
   ],
   "source": [
    "years = []\n",
    "for date in data['Date'].values:\n",
    "    year = date[-4:]\n",
    "    try:\n",
    "        years.append(int(year))\n",
    "    except:\n",
    "        print(\"got a weird version\")\n",
    "        years.append(np.nan)\n",
    "    \n",
    "data['Year'] = years \n",
    "del data['Date']\n",
    "data.fillna(method='bfill',inplace=True) #assuming its that same year, this should do it.  \n",
    "#lazy, so will leave as null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's look at latitude and longitude, and as a binary problem with a reasonable cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lons,lats,c=y,cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like lat and long doesn't play a huge role in what the magnitude is, but you can see the outlines of continents :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So now its time to build a model. With input as year, lat, and lon trying to regress to magnitude. First will be a minmax normalization and train test splitting, then the construction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y.reshape(np.shape(y)[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be changing the dataset so there is an additional hidden layer. I will try to code the NN by memory as well in order to help memorize the architecture and logic of these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = data[['Latitude','Longitude','Year']].values\n",
    "y = data['Magnitude'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the last 12 values to simplify batch size \n",
    "X = X[0:23400]\n",
    "y = y[0:23400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(np.shape(y)[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now initialize hyperparameters:\n",
    "\n",
    "num_epochs = 52560 #*10 \n",
    "#something not included can write in batches now. \n",
    "batches = 351  #not exactly sure how to set this, but if I have 351 batches that means I have a batch size of fifty. \n",
    "syn0 = 2*np.random.random((3,5)) -1 #first hidden layer's weights\n",
    "syn1 = 2 * np.random.random((5,4)) - 1  #second hidden layer's weights\n",
    "syn2 = 2 * np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4245727 ,  0.85197253,  0.15686275],\n",
       "       [ 0.59233501,  0.0690956 ,  0.35294118],\n",
       "       [ 0.69491171,  0.77806852,  0.56862745],\n",
       "       [ 0.79152894,  0.94338795,  0.35294118],\n",
       "       [ 0.38940565,  0.96290216,  0.88235294],\n",
       "       [ 0.37719794,  0.96416329,  0.        ],\n",
       "       [ 0.22042073,  0.73759285,  0.82352941],\n",
       "       [ 0.58729096,  0.21630083,  0.60784314],\n",
       "       [ 0.24674072,  0.99221931,  0.49019608],\n",
       "       [ 0.50879176,  0.4085957 ,  0.25490196]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:50]\n",
    "X_train[50*i:(50*i)+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation function goes here. \n",
    "def nonlin(x,deriv = False):\n",
    "    \"\"\"This is the sigmoid function. \n",
    "    \n",
    "    Derivative feature included to help with gradient descent to soon come. \n",
    "    \"\"\"\n",
    "    if deriv :\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4.824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-952068d3b88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mk1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#activation applied onto the first hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mk2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyn1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#activation onto the second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mk3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#activation onto the output layer, this is the resulting guess.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-a43d801d8f02>\u001b[0m in \u001b[0;36mnonlin\u001b[0;34m(x, deriv)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#activation function goes here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnonlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mderiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \"\"\"This is the sigmoid function. \n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDerivative\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mincluded\u001b[0m \u001b[0mto\u001b[0m \u001b[0mhelp\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0mto\u001b[0m \u001b[0msoon\u001b[0m \u001b[0mcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#now for training day:\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for batch in range(batches):\n",
    "        X_batch = X_train[50*i:(50*i)+50]\n",
    "        y_batch = y_train[50*i:(50*i)+50]\n",
    "        \n",
    "        k0 = X_batch\n",
    "        k1 = nonlin(np.dot(k0,syn0)) #activation applied onto the first hidden layer\n",
    "        k2 = nonlin(np.dot(k1,syn1)) #activation onto the second layer\n",
    "        k3 = nonlin(np.dot(k2,syn2)) #activation onto the output layer, this is the resulting guess. \n",
    "    \n",
    "        k3_error = y_batch - k3  # how far off you were. This is a fairl\n",
    "    \n",
    "        k3_delta = k3_error * nonlin(k3,deriv=True)\n",
    "    \n",
    "    #and backpropagation goes here. \n",
    "    #the error in hidden layer 2 is dot product of the direction of the error in the next layer, \n",
    "    #with the weights used at this particular layer. \n",
    "        k2_error = k3_delta.dot(syn2.T)\n",
    "    \n",
    "        k2_delta = k2_error * nonlin(k2,deriv=True)\n",
    "    \n",
    "        k1_error = k2_delta.dot(syn1.T)\n",
    "        k1_delta = k1_error * nonlin(k1,deriv=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    #update weights\n",
    "        syn0 += k0.T.dot(k1_delta)\n",
    "        syn1 += k1.T.dot(k2_delta)\n",
    "        syn2 += k2.T.dot(k3_delta)\n",
    "    if (i% 10000) == 0:  #quick sanity check for every 10000 epochs.  The error should drop!\n",
    "        print(\"Error: \" + str(np.mean(np.abs(k3_error))))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it runs, but it isn't learning. Based on the preliminary data exploration this seemed like it was going to be the case, but regardless going through a neural network and building it by memory was a pretty good excercise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
